#check topics
docker exec connect kafka-topics --describe --zookeeper zookeeper:2181

#Create a topic for storing data that we'll send to kafka
docker exec connect kafka-topics --create --topic quickstart-data --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:2181

#Create a directory for our input and output data files to live
docker exec connect mkdir -p /tmp/quickstart/file

#update file privileges
 chmod 744 /tmp/quickstart/file

#Create a file with dummy data for our FileSource Connector to read from
The quickstart tutorial says to use: docker exec connect bash -c 'seq 1000 > /tmp/quickstart/file/input.txt'
#move a file into the container
docker cp squence.txt connect:/input.txt

#directory
cd D:\Github\kafka-with-docker\kafka-connect

#run the twitter feed
docker exec kafka java -jar kafka_producer_twitter_jar/kafka-producer-twitter.jar

#list out topics
kafka-console-consumer  --bootstrap-server kafka:9092 --topic twitter_tweets --from-beginning


#Create a FileSource Connector to read a file from disk
docker exec connect curl -s -X POST -H "Content-Type: application/json" --data '{"name": "quickstart-file-source", "config": {"connector.class":"org.apache.kafka.connect.file.FileStreamSourceConnector", "tasks.max":"1", "topic":"quickstart-data", "file": "/input.txt"}}' http://connect:8083/connectors

#Check status of source connector
docker exec connect curl -s -X GET http://connect:8083/connectors/quickstart-file-source/status

#Read 10 records from the quickstart-data topic (using built in console consumer)
docker exec connect kafka-console-consumer --bootstrap-server kafka:9092 --topic twitter_tweets --from-beginning --max-messages 10

#Create a FileSink Connector
docker exec connect curl -X POST -H "Content-Type: application/json" --data '{"name": "quickstart-file-sink", "config": {"connector.class":"org.apache.kafka.connect.file.FileStreamSinkConnector", "tasks.max":"1", "topics":"quickstart-data", "file": "/tmp/quickstart/file/output.txt"}}' http://connect:8083/connectors

curl -X POST -H "Content-Type: application/json" --data '{"name": "twitter-sink", "config": {"connector.class":"org.apache.kafka.connect.file.FileStreamSinkConnector", "tasks.max":"1", "topics":"twitter_tweets", "file": "/connect/data/tweets.json"}}' http://connect:8083/connectors

curl -X POST -H "Content-Type: application/json" --data '{"name": "twitter-sink", "config": {"connector.class":"org.apache.kafka.connect.file.FileStreamSinkConnector", "tasks.max":"1", "topics":"twitter_tweets", "file": "tweets.txt"}}' http://connect:8083/connectors

#check file size
wc -c tweets.txt

#logs
docker logs connect --tail 50

#Check status of sink connector
docker exec connect exit

#Check that the connector worked by viewing the content of the output file
docker exec connect cat /tmp/quickstart/file/output.txt
#create a json file
echo some > ./connect/data/tweets.json


#Create a topic for the twitter messages to live
docker exec connect kafka-topics --create --topic twitter --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:2181

#Create Twitter Sink Connector
docker exec connect curl -X POST -H "Content-Type: application/json" --data '{"name":"twitter-sink","config":{"connector.class":"com.eneco.trading.kafka.connect.twitter.TwitterSinkConnector","tasks.max":1,"topics":"twitter","twitter.consumerkey":"YOUR_CONSUMER_KEY","twitter.consumersecret":"YOUR_CONSUMER_SECRET","twitter.token":"YOUR_TOKEN","twitter.secret":"YOUR_SECRET"}}' http://connect:8083/connectors

#sink commands
#whats available
curl localhost:8083/connector-plugins
#whats running
curl localhost:8083/connectors/twitter-sink/tasks
curl localhost:8083/connectors/twitter-sink/status
curl localhost:8083/connectors/twitter-sink/config
#restart
curl -X POST localhost:8083/connectors/twitter-sink/restart
#remove a connector
curl -X DELETE localhost:8083/connectors/twitter-sink

#CLEAN UP
#Delete containers created
docker rm -f $(docker ps -a -q)

#Remove unused volumes
docker volume lsdocker
docker volume prune

docker system prune

#Remove network created
docker network rm confluent

=============================
#volumes - https://www.ionos.com/community/server-cloud-infrastructure/docker/understanding-and-managing-docker-container-volumes/
# syntax
# -v [host directory]:[container directory]

#create the folder
mkdir /hostvolume

#run docker mount to the volume
docker run -it --rm --name my-directory-test -v "d:/Github/kafka-with-docker/kafka-connect/hostvolume":/containervolume centos /bin/bash

#list files, should see the files in the volume
ls /containervolume

#create a file, should see it from the file system
echo "Hello from the container." >> /containervolume/container-hello.txt

ls /hostvolume

#remove container and volume
docker rm -f -v my-directory-test