#check topics
docker exec connect kafka-topics --describe --zookeeper zookeeper:2181

#Create a topic for storing data that we'll send to kafka
docker exec connect kafka-topics --create --topic quickstart-data --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:2181

#Create a directory for our input and output data files to live
docker exec connect mkdir -p /tmp/quickstart/file

#update file privileges
 chmod 744 /tmp/quickstart/file

#Create a file with dummy data for our FileSource Connector to read from
The quickstart tutorial says to use: docker exec connect bash -c 'seq 1000 > /tmp/quickstart/file/input.txt'
#move a file into the container
docker cp squence.txt connect:/input.txt

#save an image
docker commit 3957fbdb batemansogq/cp-kafka-twitter:latest

docker push batemansogq/cp-kafka-twitter:latest


#directory
cd D:\Github\kafka-with-docker\kafka-connect
# windows envirs
set COMPOSE_CONVERT_WINDOWS_PATHS=false

#run the twitter feed
docker exec kafka java -jar kafka_producer_twitter_jar/kafka-producer-twitter.jar

#Read 10 records from the twitter topic (using built in console consumer)
docker exec connect kafka-console-consumer --bootstrap-server kafka:9092 --topic twitter_tweets --from-beginning --max-messages 10

#Create a FileSink Connector
docker exec connect curl -X POST -H "Content-Type: application/json" --data '{"name": "quickstart-file-sink", "config": {"connector.class":"org.apache.kafka.connect.file.FileStreamSinkConnector", "tasks.max":"1", "topics":"quickstart-data", "file": "/tmp/quickstart/file/output.txt"}}' http://connect:8083/connectors
#Create a direct Twitter Sink Connector
docker exec connect curl -X POST -H "Content-Type: application/json" --data '{"name":"twitter-sink","config":{"connector.class":"com.eneco.trading.kafka.connect.twitter.TwitterSinkConnector","tasks.max":1,"topics":"twitter","twitter.consumerkey":"YOUR_CONSUMER_KEY","twitter.consumersecret":"YOUR_CONSUMER_SECRET","twitter.token":"YOUR_TOKEN","twitter.secret":"YOUR_SECRET"}}' http://connect:8083/connectors


#running as a container in the background: 
 docker run -d -it connect

curl -X POST -H "Content-Type: application/json" --data '{"name": "twitter-sink", "config": {"connector.class":"org.apache.kafka.connect.file.FileStreamSinkConnector", "tasks.max":"1", "topics":"twitter_tweets", "file": "/data/tweets.json"}}' http://connect:8083/connectors

#schemas.enables =false
curl -X POST -H "Content-Type: application/json" --data '{"name": "twitter-sink", "config": {"connector.class":"org.apache.kafka.connect.file.FileStreamSinkConnector", "tasks.max":"1", "schemas.enable":false, "key.converter.schemas.enable":false, "value.converter.schemas.enable":false, "topics":"twitter_tweets", "file": "/data/tweets.json"}}' http://connect:8083/connectors

#sink commands
#whats available
curl localhost:8083/connector-plugins
#whats running
curl localhost:8083/connectors/twitter-sink/tasks
curl localhost:8083/connectors/twitter-sink/status
curl localhost:8083/connectors/twitter-sink/config

curl -X PUT localhost:8083/connectors/twitter-sink/pause
#restart
curl -X POST localhost:8083/connectors/twitter-sink/restart
#remove a connector
curl -X DELETE localhost:8083/connectors/twitter-sink




#check file size
wc -c tweets.txt

#logs
docker logs connect --tail 50

#Check status of sink connector
docker exec connect exit

#Check that the connector worked by viewing the content of the output file
docker exec connect cat /tmp/quickstart/file/output.txt
#create a json file
echo some > ./connect/data/tweets.json


#Create a topic for the twitter messages to live
docker exec connect kafka-topics --create --topic twitter --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:2181



#==================================
# confluent doco
#=================================
#docker help
https://docs.confluent.io/platform/current/installation/docker/config-reference.html#schema-registry-configuration
# full yaml
https://github.com/confluentinc/cp-demo/blob/6.0.0-post/docker-compose.yml


#CLEAN UP
#Delete containers created
docker rm -f cr$(docker ps -a -q)

#Remove unused volumes
docker volume lsdocker
docker volume prune

docker system prune

#Remove network created
docker network rm confluent

=============================
#volumes - https://www.ionos.com/community/server-cloud-infrastructure/docker/understanding-and-managing-docker-container-volumes/
# syntax
# -v [host directory]:[container directory]

#create the folder
mkdir /hostvolume
Switch
#run docker mount to the volume
docker run -it --rm --name my-directory-test -v "d:/Github/kafka-with-docker/kafka-connect/hostvolume":/containervolume centos /bin/bash

#list files, should see the files in the volume
ls /containervolume

#create a file, should see it from the file system
echo "Hello from the container." >> /containervolume/container-hello.txt

ls /hostvolume

#remove container and volume
docker rm -f -v my-directory-test